{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS - 587 : Exercise 4a ~ Transfer Learning\n",
    "## Scope:\n",
    "The goal of this assignment is to get familiar with fine-tunning in a new dataset a Convolutional Neural Network (CNN) that has been trained in another dataset, taking advantage of transfer learning.\n",
    "\n",
    "In your assignment you will be fine-tunning **AlexNet**, a popular CNN architecture, that has been pretrained on the ImageNet dataset. Your network will be finetuned for the task of recognizing art painting categories in a large dataset of art painting images, known as Wikiart.\n",
    "\n",
    "The WikiArt dataset, which consists of `3000 images of paintings` of arbitrary sizes `from 10 different styles` - Baroque, Realism, Expressionism, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 2454 images\n",
      "Test set: 610 images\n"
     ]
    }
   ],
   "source": [
    "from Utilities.wikiart_dataset import WikiArtDataset\n",
    "\n",
    "train_annotations = os.path.join('Utilities', 'data', 'train.txt')\n",
    "test_annotations = os.path.join('Utilities', 'data', 'test.txt')\n",
    "\n",
    "# use the same preprocessing as in the AlexNet model trained on ImageNet\n",
    "alexnet_weights = models.AlexNet_Weights.IMAGENET1K_V1\n",
    "preprocess = alexnet_weights.transforms()\n",
    "\n",
    "train_set = WikiArtDataset(train_annotations, transform=preprocess)\n",
    "test_set = WikiArtDataset(test_annotations, transform=preprocess)\n",
    "print(f\"Training set: {len(train_set)} images\")\n",
    "print(f\"Test set: {len(test_set)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning params\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# Network params\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Model\n",
    "For all of our image generation experiments, we will start with a convolutional neural network which was pretrained to perform image classification on ImageNet. We can use any model here, but for the purposes of this assignment we will use AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load the alexnet model pretrained on ImageNet\n",
    "alexnet = models.alexnet(weights=alexnet_weights)\n",
    "\n",
    "# freeze the feature parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "# TODO: Modify the model architecture:                                                       #\n",
    "# 1. Replace the last Fully-Connected (linear) layer with a new linear layer                 #\n",
    "# 2. Replace the last 2 Fully-Connected layers with a new linear layer                       #\n",
    "# Hint: You can access the layers of the model using alexnet.features and alexnet.classifier #\n",
    "# Hint: You can access multiple layers of the classifier using alexnet.classifier.children() #\n",
    "# Hint: You can remove layers using list slicing with the appropriate indices                #\n",
    "# Hint: You can add layers using add_module()                                                #\n",
    "##############################################################################################\n",
    "\n",
    "num_layers_to_remove = 2\n",
    "if num_layers_to_remove == 1:\n",
    "    #############################################################\n",
    "    # TODO: remove the last linear layer and add new classifier #\n",
    "    #############################################################\n",
    "    classifierLayers = list(alexnet.classifier.children())[:-1]\n",
    "    classifierLayers.append(nn.Linear(in_features=4096, out_features=num_classes))\n",
    "    alexnet.classifier = nn.Sequential(*classifierLayers)\n",
    "elif num_layers_to_remove == 2:\n",
    "    ##################################################################\n",
    "    # TODO: remove the last 2 linear layers and add a new classifier #\n",
    "    ##################################################################\n",
    "    classifierLayers = list(alexnet.classifier.children())[:-3]\n",
    "    classifierLayers.append(nn.Linear(in_features=4096, out_features=num_classes))\n",
    "    alexnet.classifier = nn.Sequential(*classifierLayers)\n",
    "\n",
    "alexnet = alexnet.to(device)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# TODO: Implement the following:                                 #\n",
    "# (a) the train and test data loaders                            #\n",
    "# (b) losss function (Soft-max Cross Entropy)                    #\n",
    "# (c) the optimization process using Stochastic Gradient Descent #\n",
    "# Create summaries in tensorboard for:                           #\n",
    "#  - the loss                                                    #\n",
    "#  - the accuracy                                                #\n",
    "##################################################################\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, alexnet.parameters()), lr=learning_rate)\n",
    "\n",
    "def accuracy(model, data_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, train accuracy: 0.276 test accuracy: 0.221\n",
      "Epoch 2/5, train accuracy: 0.339 test accuracy: 0.266\n",
      "Epoch 3/5, train accuracy: 0.415 test accuracy: 0.257\n",
      "Epoch 4/5, train accuracy: 0.447 test accuracy: 0.274\n",
      "Epoch 5/5, train accuracy: 0.520 test accuracy: 0.321\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# TODO: use ΤensorΒoard to visualize   #\n",
    "# the computational graph of the model #\n",
    "########################################\n",
    "writer = SummaryWriter(log_dir='runsFC2/transferLearning')\n",
    "\n",
    "dummyInput = torch.randn(1, 3, 224, 224).to(device)\n",
    "writer.add_graph(alexnet, dummyInput)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ###########################################################################\n",
    "        # TODO: backpropagation process (forward, loss, backward, update weights) #\n",
    "        ###########################################################################\n",
    "        outputs = alexnet(images)\n",
    "        trainLoss = loss(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        trainLoss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ##########################################################\n",
    "        # TODO: use TensorBoard to visualize for each iteration: #\n",
    "        #        - the training loss                             #\n",
    "        ##########################################################\n",
    "        globalStep = epoch * len(train_loader) + i\n",
    "        writer.add_scalar('Loss/trainIteration', trainLoss.item(), globalStep)\n",
    "\n",
    "    ###########################################################\n",
    "    # TODO: calculate the accuracy for the train and test set #\n",
    "    ###########################################################\n",
    "    acc_train = accuracy(alexnet, train_loader)\n",
    "    acc_test = accuracy(alexnet, test_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, train accuracy: {acc_train:.3f} test accuracy: {acc_test:.3f}\")\n",
    "    ######################################################\n",
    "    # TODO: use TensorBoard to visualize for each epoch: #\n",
    "    #       the train & test accuracy                    #\n",
    "    ######################################################\n",
    "    writer.add_scalar('Accuracy/trainEpoch', acc_train, epoch)\n",
    "    writer.add_scalar('Accuracy/testEpoch', acc_test, epoch)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs587",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
